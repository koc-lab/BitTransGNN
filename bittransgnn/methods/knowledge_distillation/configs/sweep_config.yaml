sweep_agent: comet
algorithm: grid
spec: 
  metric: best_val_acc #changes for different datasets
  objective: maximize
#for wandb, use the following instead of algorithm and spec:
#method: grid
#metric:
# name: best_val_acc #changes for different datasets
# goal: maximize

log_configs:
  comet: True
  pandas_df: True
  wandb: False
  project_name: bittransgnn-kd
  workspace: !!null #you should enter the name of your Comet workspace here
  api_key: !!null #you should enter the api_key of your Comet account here

load_configs:
  workspace: !!null #you should enter the name of your Comet workspace here
  api_key: !!null #you should enter the api_key of your Comet account here
  experiment_load_name: !!null
  experiment_load_key: !!null
  manual_load_ckpt: !!null
  local_load: True

experiment_configs:
  seed: [0,1,2,3,4]
  inference_type: "transductive"
  dataset_name: ["mrpc", "cola"]
  bert_pre_model: "roberta-base"
  device: "cuda:0"
  distillation_type: "offline"
  experiment_load_name: !!null
  experiment_load_key: !!null
  manual_load_ckpt: !!null
  log_ckpt: False
  save_ckpt: False
  report_time: False
  checkpoint_dir: !!null
  nb_epochs: 100
  patience: 10
  eval_test: False
  eval_test_every_n_epochs: 1

model_configs: 
  max_length: 128
  quantize_bert: True
  quantize_teacher_bert: True
  quantize_embeddings: False
  quantize_teacher_embeddings: False
  quantize_gcn: False
  num_bits_act: 8.0
  teacher_num_states: 5
  student_num_states: 5
  gcn_num_states: 2

#note that in offline distillation, teacher model parameters (e.g. lmbd) are not suitable for sweeping
parameters: 
  gcn_layers: 2
  bert_lr: 0.00001
  student_lr: 0.0001
  temperature: [1.0, 2.0]
  alpha_d: [0.3, 0.5, 0.7]
  gcn_lr: 0.001
  batch_size: 32
  teacher_lmbd: 0.4
  teacher_bert_quant_type: "PTQ"
  student_bert_quant_type: ["QAT", "PTQ"]
  joint_training: True
  graph_hidden_size: 256
  dropout: 0.5
